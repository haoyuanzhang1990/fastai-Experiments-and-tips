{"cells":[{"metadata":{},"cell_type":"markdown","source":"# fastai v2 Kernel Starter Code\n\nThe goal of this kernel is to show how to train a neural network using fastai 2.0 for this Kaggle Competition"},{"metadata":{},"cell_type":"markdown","source":"## Grabbing the Library"},{"metadata":{},"cell_type":"markdown","source":"First we need to enable internet access within this kernel and then `!git clone` the `fastai_dev` repository for us to import from."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!git clone https://github.com/fastai/fastai_dev.git\n%cd fastai_dev/dev","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're going to need a variety of imports, most importantly the `tabular.core` module for building the dataset (the rest deal with training the model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from local.data.all import *\nfrom local.tabular.core import *\nfrom local.tabular.model import *\nfrom local.optimizer import *\nfrom local.learner import *\nfrom local.metrics import *\nfrom local.callback.all import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting Up Our Data"},{"metadata":{},"cell_type":"markdown","source":"Let's make a `Path` object to our data and combine the `train.csv` with the `building_metadata.csv` to grab some more information about these meter readings. For simplicity we will use the first 1000 samples from the training set. For the `DataFrame` preperation please see ryches Kernel [here](https://www.kaggle.com/ryches/simple-lgbm-solution)"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('/kaggle/input/ashrae-energy-prediction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(path/'train.csv')\ntrain = train.iloc[:5000]\nbldg = pd.read_csv(path/'building_metadata.csv')\nweather_train = pd.read_csv(path/\"weather_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[np.isfinite(train['meter_reading'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bldg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(bldg, left_on = 'building_id', right_on = 'building_id', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(weather_train, left_on = ['site_id', 'timestamp'], right_on = ['site_id', 'timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del weather_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"hour\"] = train[\"timestamp\"].dt.hour\ntrain[\"day\"] = train[\"timestamp\"].dt.day\ntrain[\"weekend\"] = train[\"timestamp\"].dt.weekday\ntrain[\"month\"] = train[\"timestamp\"].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('timestamp', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making the DataBunch"},{"metadata":{},"cell_type":"markdown","source":"Next, just like in fastai v1 we need to declare a few things. Specifically our Categorical and Continuous variables, our preprocessors (Normalization, Categorification, and FillMissing), along with how we want to split our data. `fastai` v2 now includes a `RandomSplitter` which is similar to `.split_by_rand_pct()` but now we can specify a custom range for our data (hence `range_of(train)`)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_vars = [\"building_id\", \"primary_use\", \"hour\", \"day\", \"weekend\", \"month\", \"meter\"]\ncont_vars = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\"]\ndep_var = 'meter_reading'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"procs = [Normalize, Categorify, FillMissing]\nsplits = RandomSplitter()(range_of(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that those are defined, we can create a `TabularPandas` object by passing in our dataframe, the `procs`, our variables, what our `y` is, and how we want to split our data. `fastai` v2 is built on a Pipeline structure where first we dictate what we want to do, then we call the databunch (the high-level API is not done yet so we have nothing similar to directly DataBunching an object)"},{"metadata":{"trusted":true},"cell_type":"code","source":"to = TabularPandas(train, procs, cat_vars, cont_vars, y_names=dep_var, splits=splits)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look at what `to` actually is, we can see what looks to be a bunch of batches of our data aligned into a dataframe that can easily be read!"},{"metadata":{"trusted":true},"cell_type":"code","source":"to","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then also easily look at our training and validation datasets by calling `.train` or `.valid`"},{"metadata":{"trusted":true},"cell_type":"code","source":"to.train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can create our DataBunch object one of two ways. We can either directly do a `dbch = to.databunch()`, *or* we can take it one step further and apply custom works to some dataloaders. First let's look at the basic version"},{"metadata":{"trusted":true},"cell_type":"code","source":"dbch = to.databunch()\ndbch.valid_dl.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try doing this the second method. We can increase our batch size since the validation set is much smaller than our training dataset. We can also specify a few options with our training dataset too. To do this, we will need to create `TabDataLoaders` to, well, load the data!\n\nWe pass in a dataset, a batch size, our `num_workers`, along with if we want to shuffle our dataset and drop the last batch if it does not evenly split. You should always want to do this with the **training** dataset but not the validation. Defaultly they are both set to `False`"},{"metadata":{"trusted":true},"cell_type":"code","source":"trn_dl = TabDataLoader(to.train, bs=64, num_workers=0, shuffle=True, drop_last=True)\nval_dl = TabDataLoader(to.valid, bs=128, num_workers=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly we can create a `DataBunch` object by calling `DataBunch()` and passing in our two `DataLoaders`"},{"metadata":{"trusted":true},"cell_type":"code","source":"dbunch = DataBunch(trn_dl, val_dl)\ndbunch.valid_dl.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there are a *lot* of ways we can customize our DataBunch's now"},{"metadata":{},"cell_type":"markdown","source":"## Training the Model"},{"metadata":{},"cell_type":"markdown","source":"First we need to create a `TabularModel` that needs an embedding matrix size, how many continuous variables to expect, the number of possible outputs (classes), and how big we want our layers. To pass in the embedding matrix sizes, we can use `get_emb_sz` onto a `TabularPandas` object"},{"metadata":{},"cell_type":"markdown","source":"First let's define our embedding size rule of thumb, along with our `get_emb_sz` function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def emb_sz_rule(n_cat): \n    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n    return min(600, round(1.6 * n_cat**0.56))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _one_emb_sz(classes, n, sz_dict=None):\n    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n    sz_dict = ifnone(sz_dict, {})\n    n_cat = len(classes[n])\n    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n    return n_cat,sz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_emb_sz(to, sz_dict=None):\n    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n    return [_one_emb_sz(to.procs.classes, n, sz_dict) for n in to.cat_names]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we pass in our `TabularPandas` object, `to`"},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_szs = get_emb_sz(to); print(emb_szs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last piece of the puzzle we need is our basic `TabularModel`"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0., y_range=None, use_bn=True, bn_final=False):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n        _layers = [BnDropLin(sizes[i], sizes[i+1], bn=use_bn and i!=0, p=p, act=a)\n                       for i,(p,a) in enumerate(zip([0.]+ps,actns))]\n        if bn_final: _layers.append(nn.BatchNorm1d(sizes[-1]))\n        self.layers = nn.Sequential(*_layers)\n    \n    def forward(self, x_cat, x_cont):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        x = self.layers(x)\n        if self.y_range is not None:\n            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you noticed, most of what changed with the v2 API is focused on the dataloading / DataBunch creation. The rest of this Kernel sould look very familiar to fastai users"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = TabularModel(emb_szs, len(to.cont_names), 1, [1000,500]); model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can define our optimization function and create our `Learner`"},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_func = partial(Adam, wd=0.01, eps=1e-5)\nlearn = Learner(dbunch, model, MSELossFlat(), opt_func=opt_func)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I need to solve the bug for why we are not fitting properly, but this is also just a subset of the data. Hope this helps you get started! :)\n\n- muellerzr"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}